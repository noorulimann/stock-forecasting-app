"""
Model Comparison and Benchmarking Utilities

This module provides comprehensive comparison functionality for evaluating
different models in the ML pipeline.
import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
import json

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelComparator:
    """
    Compare performance across different model types
    Highlights LightGBM advantages in the ML pipeline
    """
    
    def __init__(self):
        self.comparison_history = []
        self.benchmark_results = {}
    
    def compare_models(self, results: Dict[str, Dict[str, Any]], 
                      symbol: str = None) -> pd.DataFrame:
        """
        Create comprehensive comparison table across all models
        
        Args:
            results: Dictionary of model results with metrics
            symbol: Optional symbol for context
            
        Returns:
            DataFrame with comparison metrics
        """
        try:
            logger.info("ðŸ“Š Creating model comparison table...")
            
            comparison_data = []
            
            for model_name, model_results in results.items():
                if isinstance(model_results, dict):
                    metrics = model_results.get('metrics', model_results)
                    
                    row = {
                        'Model': model_name.upper(),
                        'RMSE': metrics.get('rmse', np.nan),
                        'MAE': metrics.get('mae', np.nan),
                        'RÂ²': metrics.get('r2', np.nan),
                        'MAPE (%)': metrics.get('mape', np.nan),
                        'Dir. Accuracy (%)': metrics.get('directional_accuracy', np.nan),
                        'Training Time (s)': model_results.get('training_time', 
                                                              model_results.get('training_history', {}).get('training_time', np.nan))
                    }
                    
                    comparison_data.append(row)
            
            if not comparison_data:
                logger.warning("âš ï¸ No model data available for comparison")
                return pd.DataFrame()
            
            # Create DataFrame
            df = pd.DataFrame(comparison_data)
            
            # Sort by RMSE (lower is better)
            df = df.sort_values('RMSE')
            
            # Add ranking
            df.insert(0, 'Rank', range(1, len(df) + 1))
            
            # Format numeric columns
            for col in ['RMSE', 'MAE', 'RÂ²', 'MAPE (%)', 'Dir. Accuracy (%)', 'Training Time (s)']:
                if col in df.columns:
                    df[col] = df[col].round(4)
            
            # Store comparison
            self.comparison_history.append({
                'timestamp': datetime.now().isoformat(),
                'symbol': symbol,
                'comparison': df.to_dict('records')
            })
            
            logger.info(f"âœ… Comparison complete - {len(df)} models compared")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ Error creating comparison: {e}")
            return pd.DataFrame()
    
    def highlight_best_model(self, comparison_df: pd.DataFrame) -> Dict[str, Any]:
        """
        Identify and highlight the best performing model
        
        Args:
            comparison_df: Comparison DataFrame
            
        Returns:
            Dictionary with best model information
        """
        try:
            if comparison_df.empty:
                return {}
            
            # Best overall (lowest RMSE)
            best_row = comparison_df.iloc[0]
            
            # Calculate improvement over others
            improvements = {}
            
            if len(comparison_df) > 1:
                best_rmse = best_row['RMSE']
                
                for idx, row in comparison_df.iterrows():
                    if row['Model'] != best_row['Model']:
                        if not np.isnan(row['RMSE']) and row['RMSE'] > 0:
                            improvement = ((row['RMSE'] - best_rmse) / row['RMSE']) * 100
                            improvements[row['Model']] = improvement
            
            result = {
                'best_model': best_row['Model'],
                'metrics': {
                    'rmse': best_row['RMSE'],
                    'mae': best_row['MAE'],
                    'r2': best_row['RÂ²'],
                    'mape': best_row['MAPE (%)'],
                    'directional_accuracy': best_row['Dir. Accuracy (%)']
                },
                'improvements_over': improvements,
                'rank': 1
            }
            
            logger.info(f"ðŸ† Best model: {best_row['Model']}")
            logger.info(f"   RMSE: {best_row['RMSE']:.4f}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Error highlighting best model: {e}")
            return {}
    
    def benchmark_training_speed(self, model_results: Dict[str, Dict[str, Any]]) -> pd.DataFrame:
        """
        Benchmark training speed across models
        
        Args:
            model_results: Dictionary of model training results
            
        Returns:
            DataFrame with speed comparison
        """
        try:
            logger.info("âš¡ Benchmarking training speed...")
            
            speed_data = []
            
            for model_name, results in model_results.items():
                training_time = results.get('training_history', {}).get('training_time', 0)
                
                if training_time > 0:
                    samples = results.get('training_history', {}).get('train_samples', 0)
                    
                    speed_data.append({
                        'Model': model_name.upper(),
                        'Training Time (s)': training_time,
                        'Samples': samples,
                        'Samples/Second': samples / training_time if training_time > 0 else 0
                    })
            
            df = pd.DataFrame(speed_data)
            
            if not df.empty:
                df = df.sort_values('Training Time (s)')
                df['Speed Rank'] = range(1, len(df) + 1)
            
            logger.info(f"âœ… Speed benchmark complete - {len(df)} models")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ Error benchmarking speed: {e}")
            return pd.DataFrame()
    
    def analyze_lightgbm_advantages(self, lightgbm_result: Dict[str, Any],
                                   other_models: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analyze and highlight LightGBM's advantages over other models
        
        Args:
            lightgbm_result: LightGBM training results
            other_models: Results from other models
            
        Returns:
            Analysis dictionary highlighting advantages
        """
        try:
            logger.info("ðŸŒŸ Analyzing LightGBM advantages...")
            
            advantages = {
                'model': 'LightGBM',
                'advantages': [],
                'metrics_comparison': {},
                'feature_insights': {}
            }
            
            lgb_metrics = lightgbm_result.get('metrics', {})
            
            # Compare with each model
            for model_name, model_result in other_models.items():
                other_metrics = model_result.get('metrics', {})
                
                # RMSE comparison
                if 'rmse' in lgb_metrics and 'rmse' in other_metrics:
                    lgb_rmse = lgb_metrics['rmse']
                    other_rmse = other_metrics['rmse']
                    
                    if lgb_rmse < other_rmse:
                        improvement = ((other_rmse - lgb_rmse) / other_rmse) * 100
                        advantages['advantages'].append(
                            f"{improvement:.1f}% better RMSE than {model_name.upper()}"
                        )
            
            # Feature importance insights
            if 'feature_importance' in lightgbm_result:
                feature_importance = lightgbm_result['feature_importance']
                top_features = list(feature_importance.keys())[:5]
                
                advantages['feature_insights'] = {
                    'top_5_features': top_features,
                    'interpretability': 'High - provides feature importance rankings',
                    'feature_count': len(feature_importance)
                }
                
                advantages['advantages'].append(
                    f"Provides interpretable feature importance for {len(feature_importance)} features"
                )
            
            # Cross-validation insights
            if 'cross_validation' in lightgbm_result:
                cv_results = lightgbm_result['cross_validation']
                if cv_results:
                    advantages['advantages'].append(
                        f"Validated with {cv_results.get('n_splits', 5)}-fold cross-validation"
                    )
                    advantages['advantages'].append(
                        f"Consistent performance: RMSE {cv_results.get('mean_rmse', 0):.4f} Â± {cv_results.get('std_rmse', 0):.4f}"
                    )
            
            # General LightGBM advantages
            advantages['advantages'].extend([
                "Fast training speed with gradient boosting",
                "Handles missing values automatically",
                "Built-in regularization prevents overfitting",
                "Efficient memory usage for large datasets",
                "Industry-standard for production ML systems"
            ])
            
            advantages['metrics_comparison'] = lgb_metrics
            
            logger.info(f"âœ… LightGBM analysis complete - {len(advantages['advantages'])} advantages identified")
            
            return advantages
            
        except Exception as e:
            logger.error(f"âŒ Error analyzing LightGBM advantages: {e}")
            return {}
    
    def generate_comparison_report(self, all_results: Dict[str, Any],
                                  symbol: str) -> Dict[str, Any]:
        """
        Generate comprehensive comparison report
        
        Args:
            all_results: All model training results
            symbol: Stock symbol
            
        Returns:
            Comprehensive comparison report
        """
        try:
            logger.info(f"ðŸ“‹ Generating comparison report for {symbol}...")
            
            report = {
                'symbol': symbol,
                'timestamp': datetime.now().isoformat(),
                'models_compared': [],
                'summary': {},
                'detailed_comparison': None,
                'lightgbm_analysis': None,
                'recommendations': []
            }
            
            # Extract metrics from all models
            model_metrics = {}
            
            # Traditional models
            if 'traditional' in all_results:
                for model_name, success in all_results['traditional'].items():
                    if success:
                        model_metrics[f'traditional_{model_name}'] = {
                            'metrics': {}  # Would need actual metrics
                        }
            
            # LightGBM
            if 'lightgbm' in all_results and all_results['lightgbm'].get('success'):
                model_metrics['lightgbm'] = all_results['lightgbm']
                report['models_compared'].append('LightGBM')
            
            # Neural models
            if 'neural' in all_results and all_results['neural'].get('success'):
                for model_type in all_results['neural'].get('models_trained', []):
                    model_metrics[f'neural_{model_type}'] = {
                        'metrics': {}
                    }
                    report['models_compared'].append(f'Neural_{model_type}')
            
            # Create comparison table
            comparison_df = self.compare_models(model_metrics, symbol)
            
            if not comparison_df.empty:
                report['detailed_comparison'] = comparison_df.to_dict('records')
                report['summary'] = self.highlight_best_model(comparison_df)
            
            # LightGBM-specific analysis
            if 'lightgbm' in model_metrics:
                other_models = {k: v for k, v in model_metrics.items() if k != 'lightgbm'}
                report['lightgbm_analysis'] = self.analyze_lightgbm_advantages(
                    model_metrics['lightgbm'],
                    other_models
                )
            
            # Recommendations
            if report['summary'].get('best_model') == 'LIGHTGBM':
                report['recommendations'].extend([
                    "âœ… LightGBM recommended for production deployment",
                    "âœ… Provides best balance of accuracy and speed",
                    "âœ… Feature importance available for model interpretability",
                    "âœ… Well-suited for end-to-end ML pipeline"
                ])
            
            logger.info(f"âœ… Comparison report generated for {symbol}")
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ Error generating comparison report: {e}")
            import traceback
            traceback.print_exc()
            return {
                'error': str(e),
                'symbol': symbol
            }
    
    def export_comparison_to_json(self, report: Dict[str, Any], 
                                 filepath: str = None) -> bool:
        """
        Export comparison report to JSON file
        
        Args:
            report: Comparison report
            filepath: Output file path
            
        Returns:
            Success status
        """
        try:
            if filepath is None:
                filepath = f"comparison_report_{report.get('symbol', 'unknown')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(filepath, 'w') as f:
                json.dump(report, f, indent=2, default=str)
            
            logger.info(f"âœ… Comparison report exported to {filepath}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error exporting report: {e}")
            return False


# Singleton instance
_model_comparator_instance = None

def get_model_comparator() -> ModelComparator:
    """Get singleton instance of ModelComparator"""
    global _model_comparator_instance
    if _model_comparator_instance is None:
        _model_comparator_instance = ModelComparator()
    return _model_comparator_instance


if __name__ == "__main__":
    # Demo usage
    logger.info("Model Comparator Module - Demo")
    comparator = get_model_comparator()
    logger.info("âœ… Module ready for use")
